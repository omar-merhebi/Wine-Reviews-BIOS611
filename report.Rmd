---
title: "Predicting Wine Quality and Price"
author: "Omar Merhebi"
date: "`r Sys.Date()`"
output:
  pdf_document:
    extra_dependencies: ["float"]
---

## Introduction
For this project, I am attempting to see if we can use a relatively simple neural network to predict the price and quality of wine based on a text-based description by a reviewer as well as some other characteristics such as its year and country/region of origin. The quality in this dataset is determined by the number of points assigned by Wine Magazine. My hypothesis is that we will be able to create two neural nets, each with high predicitive power of price and quality using the tokenized description as well as the other variables from the dataset. 

## Preliminary Data Exploration
First, we want to take a look at overall trends and patterns in this dataset. First, I will take a look at the distribution of the target variables (price and points). First let's take a look at points:

```{r, fig.pos="H", out.width="60%", echo=FALSE, fig.align="center", fig.cap='Barplot of wine counts by continent.'}
knitr::include_graphics('./figures/histogram-by-points.png')
```

We can see that the point values for the wines are pretty normally distributed, although they don't span the full range of point values (they go from 80 to ~95). This is likely because this dataset comes from Wine Magazine and I believe that they only include wines that they recommend in this dataset. Moving on let's take a look at the price distribution, which has a much wider range.

```{r, fig.show='hold', out.width='50%', echo=FALSE, fig.cap='Histograms of wine prices with outliers included (left) and with outliers removed (right).', fig.pos="H"}
knitr::include_graphics('./figures/histogram-by-price-full.png')
knitr::include_graphics('./figures/histogram-by-price-no-outliers.png')
```

As can be seen, the range of wine prices is much much larger, with some wines costing over $3,000 a bottle! These higher price wines though, are quite rare and won't be useful in the training set of our neural network. As such, we remove the outliers. The threshold for removing outliers is defined as the following:


$$outliers > Q_3 + 1.5 \times IQR \\$$

where

$IQR = Q_3 - Q_1$

$Q_3 = \text{third quartile}$, and

$Q_1 = \text{first quartile}$

$\text{ }$

The first and third quartile are defined as the values below which 25% and 75% of price values fall, respectively. After removing the outliers, we see the prices have a distribution that is skewed left, towards cheaper wines. 


Finally, I wanted to see if there was bias towards a specific region of thw world in this dataset, as I suspected the distributions of wine in this dataset would not be balanced. The original dataset listed the wines by country, but there are way too many to try to plot the counts of wines from each country, and I also thought that grouping the wines by continent would give a better idea as to which regions are over- and under-represented. The distribution of wines by continent is shown below.

```{r, fig.pos="H", fig.align="center", out.width="50%", echo=FALSE, fig.cap='Barplot of wine counts by continent.'}
knitr::include_graphics('./figures/histogram-by-continent.png')
```

I did expect this data to be imbalanced and show more wines from Europe and North America, as these are areas with extensive wine industries (think Napa, France, Italy, etc.) but I did not expect the imbalance to be so drastic. If this was not a project for a course and I was trying to seriously develop a neural network to make predictions on this data, this would a problem and I would have to do some over- or under-sampling of wines to balance out this data. But, we are just going to carry forward and it'll be interesting to see if this data imbalance carries over in how the model performs. 

## Word Tokenization and Principal Component Analysis (PCA)

The next step is to convert the text based description of the wine by the reviewer into something we can feed into a neural network. To do this, I used `nltk`, a Python package for natural language processing, to tokenize the words in the description, filter out common words like "the", "to", and "at". I also removed any numbers found in the text as this usually signified a year and that information was already contained in the original data, and I removed common words that did not add any information to the description like "wine", "blend", "finish", "texture". These are all words that are used to describe wine but it's actually the adjectives that precede these words that should be important for prediction. 

Finally, I also lemmatized the words so that similar words were grouped together. For example, I wanted to make sure that if one description says "this wine has a hint of cherry" and another says "there are notes of cherries" that both "cherry" and "cherries" counted as the same word. After tokenization, I kept only words that had a word count of at least 1000 (with over 80,000 wines in this dataset after filtering down, I believe that's a generous threshold). This also follows with words like "beautiful" and "beauty". After performing this processing, we can see the top 25 most used words in the dataset to describe these wines:

```{r, fig.pos="H", fig.align="center", out.width="70%", echo=FALSE, fig.cap='Most common words in the dataset.'}
knitr::include_graphics('./figures/top-25-words.png')
```

The most common words in this dataset include words like fruit (which has a much greater count than all the other most common words), tannin, spice, and herb. All of the top 25 words shown are words that are characteristic of wine reviews so this is a good sign that the tokenization worked well.

Now, I am going to attempt to perform PCA on the tokenized words and see if we can start to show distinct groupings based on continent.

```{r, fig.pos="H", fig.align="center", out.width="70%", echo=FALSE, fig.cap='PCA of tokenized data, colored by continent.'}
knitr::include_graphics('./figures/pca-by-continent.png')
```

It doesn't look like PCA helped resolve any differences across wine regions, but I honestly expected this due to factors like the data imbalance, sheer size and variance of this dataset, and the amount of words still kept. So, let's now move on to training a neural network. 

## Neural Network Results
I went ahead and trained two regression neural networks: one to predict price and the other, points. Both neural networks have the same architecture which looks like:

```{r, fig.pos="H", fig.align="center", out.width="30%", echo=FALSE, fig.cap='Neural Network Architecture'}
knitr::include_graphics('./figures/nn-diagram.png')
```

The networks each have three layers (besides the input layer) with a decreasing number of neurons in each subsequent layer, with one output neuron. Before training the networks, categorical variables in the data were one-hot encoded as this is standard practice in machine learning. The "Year" feature in this dataset was considered to be categorical, not numeric. The dataset was split into 80% training and 20% testing datasets and 20% of the training set was used as a validation datset during model training. 

First, here are scatter plots showing the predicted price/points vs. the true price/points for the test set:

```{r, fig.show='hold', fig.pos="H", out.width="50%", echo=FALSE, fig.cap='Scatterplot of predicted vs true price (left) and points (right) on the test set. The identity line signifies perfect predictions and is plotted in red. The mean absolute error (MAE) of each is shown at the top right.'}
knitr::include_graphics('./figures/price_prediction_scatterplot.png')
knitr::include_graphics('./figures/points_prediction_scatterplot.png')
```

The neural networks did okay, but not amazingly. We can see from the high degree of variance in predictions that it wasn't able to make very accurate assessments of the price or quality of the wine based on the descriptions. The MAEs are actually not too bad but I anticipate this is because the errors seemingly "average out" to a value close to zero and not because the neural network performed well. We can check to see if there were any biases in how well the neural network predicted points and price based on the region that the wine is from:

```{r, fig.show='hold', fig.pos="H", out.width="50%", echo=FALSE, fig.cap='Boxplots showing the distribution of predicted vs true price (left) and points (right) on the test set broken down by continent of origin of the wine.'}
knitr::include_graphics('./figures/price-boxplots.png')
knitr::include_graphics('./figures/points-boxplots.png')
```

So starting with price, we can tell that the price prediction distribution actually matched up with the true distribution quite well and there didn't seem to be any bias by continent. For points, there was also not any bias by continent but there were major issues with this prediction, namely:

The median of the distribution of predicted points was consistently much lower than the true median, and quite strikingly, the range of predicted points was much larger than the true range. The model predicted wines having a point value of over 200 when it's not even possible for a wine to have a value of greater than 100. There is something seriously wrong with this feature, either inherently in the dataset or in the construction of the neural network. I am not sure if the limited range of point values in the original dataset could have caused this. 

## Conclusion

Overall, this dataset may not have been ideal or may have required further preprocessing to build a reliable neural network capable of making predictions. Or, it's quite possible that wine reviewer's reviews don't at all reflect the quality or overall cost of the wine. It's quite possible that reviewers tend to highlight the positives and be over-complimentary and thus making it difficult to draw quantitative conclusions from their reviews. 