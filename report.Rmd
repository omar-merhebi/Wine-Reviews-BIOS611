---
title: "Predicting Wine Quality and Price"
author: "Omar Merhebi"
date: "`r Sys.Date()`"
output:
  pdf_document:
    extra_dependencies: ["float"]
---

## Introduction
For this project, I am attempting to see if we can use a relatively simple neural network to predict the price and quality of wine based on a text-based description by a reviewer as well as some other characteristics such as its year and country/region of origin. The quality in this dataset is determined by the number of points assigned by Wine Magazine. My hypothesis is that we will be able to create two neural nets, each with high predicitive power of price and quality using the tokenized description as well as the other variables from the dataset. 

## Preliminary Data Exploration
First, we want to take a look at overall trends and patterns in this dataset. First, I will take a look at the distribution of the target variables (price and points). First let's take a look at points:

```{r, fig.pos="H", out.width="70%", echo=FALSE, fig.align="center", fig.cap='Barplot of wine counts by continent.'}
knitr::include_graphics('./figures/histogram-by-points.png')
```

We can see that the point values for the wines are pretty normally distributed, although they don't span the full range of point values (they go from 80 to ~95). This is likely because this dataset comes from Wine Magazine and I believe that they only include wines that they recommend in this dataset. Moving on let's take a look at the price distribution, which has a much wider range.

```{r, fig.show='hold', out.width='50%', echo=FALSE, fig.cap='Histograms of wine prices with outliers included (left) and with outliers removed (right).', fig.pos="H"}
knitr::include_graphics('./figures/histogram-by-price-full.png')
knitr::include_graphics('./figures/histogram-by-price-no-outliers.png')
```

As can be seen, the range of wine prices is much much larger, with some wines costing over $3,000 a bottle! These higher price wines though, are quite rare and won't be useful in the training set of our neural network. As such, we remove the outliers. The threshold for removing outliers is defined as the following:


$$outliers > Q_3 + 1.5 \times IQR \\$$

where

$IQR = Q_3 - Q_1$

$Q_3 = \text{third quartile}$, and

$Q_1 = \text{first quartile}$

$\text{ }$

The first and third quartile are defined as the values below which 25% and 75% of price values fall, respectively. After removing the outliers, we see the prices have a distribution that is skewed left, towards cheaper wines. 


Finally, I wanted to see if there was bias towards a specific region of thw world in this dataset, as I suspected the distributions of wine in this dataset would not be balanced. The original dataset listed the wines by country, but there are way too many to try to plot the counts of wines from each country, and I also thought that grouping the wines by continent would give a better idea as to which regions are over- and under-represented. The distribution of wines by continent is shown below.

```{r, fig.pos="H", fig.align="center", out.width="50%", echo=FALSE, fig.cap='Barplot of wine counts by continent.'}
knitr::include_graphics('./figures/histogram-by-continent.png')
```

I did expect this data to be imbalanced and show more wines from Europe and North America, as these are areas with extensive wine industries (think Napa, France, Italy, etc.) but I did not expect the imbalance to be so drastic. If this was not a project for a course and I was trying to seriously develop a neural network to make predictions on this data, this would a problem and I would have to do some over- or under-sampling of wines to balance out this data. But, we are just going to carry forward and it'll be interesting to see if this data imbalance carries over in how the model performs. 

## Word Tokenization and Principal Component Analysis (PCA)

The next step is to convert the text based description of the wine by the reviewer into something we can feed into a neural network. To do this, I used `nltk`, a Python package for natural language processing, to tokenize the words in the description, filter out common words like "the", "to", and "at". I also removed any numbers found in the text as this usually signified a year and that information was already contained in the original data, and I removed common words that did not add any information to the description like "wine", "blend", "finish", "texture". These are all words that are used to describe wine but it's actually the adjectives that precede these words that should be important for prediction. 

Finally, I also lemmatized the words so that similar words were grouped together. For example, I wanted to make sure that if one description says "this wine has a hint of cherry" and another says "there are notes of cherries" that both "cherry" and "cherries" counted as the same word. After tokenization, I kept only words that had a word count of at least 1000 (with over 80,000 wines in this dataset after filtering down, I believe that's a generous threshold). This also follows with words like "beautiful" and "beauty". After performing this processing, we can see the top 25 most used words in the dataset to describe these wines:

```{r, fig.pos="H", fig.align="center", out.width="70%", echo=FALSE, fig.cap='Most common words in the dataset.'}
knitr::include_graphics('./figures/top-25-words.png')
```

The most common words in this dataset include words like fruit (which has a much greater count than all the other most common words), tannin, spice, and herb. All of the top 25 words shown are words that are characteristic of wine reviews so this is a good sign that the tokenization worked well.

Now, I am going to attempt to perform PCA on the tokenized words and see if we can start to show distinct groupings based on continent.

```{r, fig.pos="H", fig.align="center", out.width="70%", echo=FALSE, fig.cap='PCA of tokenized data, colored by continent.'}
knitr::include_graphics('./figures/pca-by-continent.png')
```

It doesn't look like PCA helped resolve any differences across wine regions, but I honestly expected this due to factors like the data imbalance, sheer size and variance of this dataset, and the amount of words still kept. So, let's now move on to training a neural network. 